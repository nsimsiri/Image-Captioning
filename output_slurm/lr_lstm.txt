Namespace(batch_size=25, caption_path='./coco/annotations/sm_captions_train2014.json', crop_size=224, embed_size=256, hidden_size=512, image_dir='./data/resized2014', learning_rate=1e-05, log_step=50, model_path='./models/EXP2_LR', num_epochs=5, num_layers=1, num_workers=2, save_step=500, vocab_path='./data/vocab.pkl')
loading annotations into memory...
0:00:00.103114
creating index...
index created!
----- DATA_LOADER -- loaded data ----
---- USING GPU ---- 
train_exp_lr.py:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  return Variable(x, volatile=volatile)
train_exp_lr.py:96: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  loss.data[0], np.exp(loss.data[0]), time.time()-t0))
Epoch [0/5], Step [0/1001], Loss: 9.2088, Perplexity: 9984.9346, Time: 0.4520
Epoch [0/5], Step [50/1001], Loss: 9.1890, Perplexity: 9789.3438, Time: 10.0899
Epoch [0/5], Step [100/1001], Loss: 9.1705, Perplexity: 9609.1602, Time: 19.7487
Epoch [0/5], Step [150/1001], Loss: 9.1458, Perplexity: 9374.6748, Time: 29.5805
Epoch [0/5], Step [200/1001], Loss: 9.1099, Perplexity: 9043.9463, Time: 39.3268
Epoch [0/5], Step [250/1001], Loss: 9.0580, Perplexity: 8587.2842, Time: 49.1524
Epoch [0/5], Step [300/1001], Loss: 8.8091, Perplexity: 6694.7197, Time: 59.0444
Epoch [0/5], Step [350/1001], Loss: 7.7166, Perplexity: 2245.3938, Time: 68.9299
Epoch [0/5], Step [400/1001], Loss: 7.2201, Perplexity: 1366.5709, Time: 79.4275
Epoch [0/5], Step [450/1001], Loss: 6.9362, Perplexity: 1028.8759, Time: 89.9491
Epoch [0/5], Step [500/1001], Loss: 6.5800, Perplexity: 720.5179, Time: 100.1780
Epoch [0/5], Step [550/1001], Loss: 6.4667, Perplexity: 643.3845, Time: 110.5574
Epoch [0/5], Step [600/1001], Loss: 6.3301, Perplexity: 561.1913, Time: 120.9409
Epoch [0/5], Step [650/1001], Loss: 6.3034, Perplexity: 546.4527, Time: 131.3381
Epoch [0/5], Step [700/1001], Loss: 6.1909, Perplexity: 488.2715, Time: 141.8383
Epoch [0/5], Step [750/1001], Loss: 5.9351, Perplexity: 378.0758, Time: 152.2876
Epoch [0/5], Step [800/1001], Loss: 6.0117, Perplexity: 408.1804, Time: 162.7648
Epoch [0/5], Step [850/1001], Loss: 6.2622, Perplexity: 524.3615, Time: 173.3197
Epoch [0/5], Step [900/1001], Loss: 5.7268, Perplexity: 306.9862, Time: 183.8136
Epoch [0/5], Step [950/1001], Loss: 5.7224, Perplexity: 305.6417, Time: 194.2764
Epoch [0/5], Step [1000/1001], Loss: 5.6837, Perplexity: 294.0399, Time: 204.7389
Epoch [1/5], Step [0/1001], Loss: 5.4777, Perplexity: 239.2920, Time: 205.3347
Epoch [1/5], Step [50/1001], Loss: 5.5514, Perplexity: 257.5891, Time: 215.8140
Epoch [1/5], Step [100/1001], Loss: 5.2948, Perplexity: 199.2878, Time: 226.3183
Epoch [1/5], Step [150/1001], Loss: 5.7717, Perplexity: 321.0921, Time: 236.8688
Epoch [1/5], Step [200/1001], Loss: 5.5698, Perplexity: 262.3849, Time: 247.3856
Epoch [1/5], Step [250/1001], Loss: 5.3821, Perplexity: 217.4770, Time: 257.9243
Epoch [1/5], Step [300/1001], Loss: 5.4551, Perplexity: 233.9498, Time: 268.4757
Epoch [1/5], Step [350/1001], Loss: 5.4353, Perplexity: 229.3668, Time: 279.0211
Epoch [1/5], Step [400/1001], Loss: 5.3563, Perplexity: 211.9451, Time: 289.5124
Epoch [1/5], Step [450/1001], Loss: 5.2610, Perplexity: 192.6701, Time: 299.9960
Epoch [1/5], Step [500/1001], Loss: 5.2871, Perplexity: 197.7749, Time: 310.4975
Epoch [1/5], Step [550/1001], Loss: 5.2322, Perplexity: 187.2038, Time: 320.9930
Epoch [1/5], Step [600/1001], Loss: 5.0838, Perplexity: 161.3835, Time: 331.4851
Epoch [1/5], Step [650/1001], Loss: 5.2903, Perplexity: 198.3973, Time: 341.9828
Epoch [1/5], Step [700/1001], Loss: 5.2509, Perplexity: 190.7306, Time: 352.5062
Epoch [1/5], Step [750/1001], Loss: 4.9335, Perplexity: 138.8708, Time: 363.0007
Epoch [1/5], Step [800/1001], Loss: 5.0426, Perplexity: 154.8735, Time: 373.4685
Epoch [1/5], Step [850/1001], Loss: 4.8936, Perplexity: 133.4320, Time: 383.9544
Epoch [1/5], Step [900/1001], Loss: 4.9598, Perplexity: 142.5675, Time: 394.4503
Epoch [1/5], Step [950/1001], Loss: 5.2462, Perplexity: 189.8437, Time: 404.9518
Epoch [1/5], Step [1000/1001], Loss: 4.8369, Perplexity: 126.0813, Time: 415.3593
Epoch [2/5], Step [0/1001], Loss: 4.8376, Perplexity: 126.1654, Time: 415.9309
Epoch [2/5], Step [50/1001], Loss: 5.0311, Perplexity: 153.0942, Time: 426.3598
Epoch [2/5], Step [100/1001], Loss: 4.8169, Perplexity: 123.5798, Time: 436.8580
Epoch [2/5], Step [150/1001], Loss: 4.8405, Perplexity: 126.5263, Time: 447.3644
Epoch [2/5], Step [200/1001], Loss: 4.7513, Perplexity: 115.7390, Time: 457.8401
Epoch [2/5], Step [250/1001], Loss: 4.9486, Perplexity: 140.9741, Time: 468.2979
Epoch [2/5], Step [300/1001], Loss: 4.7133, Perplexity: 111.4147, Time: 478.7844
Epoch [2/5], Step [350/1001], Loss: 4.6697, Perplexity: 106.6659, Time: 489.3133
Epoch [2/5], Step [400/1001], Loss: 4.7161, Perplexity: 111.7262, Time: 499.7777
Epoch [2/5], Step [450/1001], Loss: 4.8155, Perplexity: 123.4138, Time: 510.2053
Epoch [2/5], Step [500/1001], Loss: 5.0194, Perplexity: 151.3261, Time: 520.6459
Epoch [2/5], Step [550/1001], Loss: 4.8391, Perplexity: 126.3498, Time: 531.0930
Epoch [2/5], Step [600/1001], Loss: 4.9612, Perplexity: 142.7635, Time: 541.5063
Epoch [2/5], Step [650/1001], Loss: 4.8372, Perplexity: 126.1161, Time: 551.8805
Epoch [2/5], Step [700/1001], Loss: 4.7427, Perplexity: 114.7477, Time: 562.2758
Epoch [2/5], Step [750/1001], Loss: 4.7219, Perplexity: 112.3841, Time: 572.7154
Epoch [2/5], Step [800/1001], Loss: 4.7156, Perplexity: 111.6743, Time: 583.1380
Epoch [2/5], Step [850/1001], Loss: 4.6210, Perplexity: 101.5961, Time: 593.4969
Epoch [2/5], Step [900/1001], Loss: 4.7118, Perplexity: 111.2487, Time: 603.9178
Epoch [2/5], Step [950/1001], Loss: 4.4107, Perplexity: 82.3289, Time: 614.3094
Epoch [2/5], Step [1000/1001], Loss: 4.7542, Perplexity: 116.0746, Time: 624.6484
Epoch [3/5], Step [0/1001], Loss: 4.8520, Perplexity: 127.9938, Time: 625.2092
Epoch [3/5], Step [50/1001], Loss: 4.8732, Perplexity: 130.7355, Time: 635.5856
Epoch [3/5], Step [100/1001], Loss: 4.7467, Perplexity: 115.2030, Time: 645.9697
Epoch [3/5], Step [150/1001], Loss: 4.8210, Perplexity: 124.0924, Time: 656.3761
Epoch [3/5], Step [200/1001], Loss: 4.9555, Perplexity: 141.9604, Time: 666.8211
Epoch [3/5], Step [250/1001], Loss: 4.4571, Perplexity: 86.2362, Time: 677.2333
Epoch [3/5], Step [300/1001], Loss: 4.7034, Perplexity: 110.3165, Time: 687.6263
Epoch [3/5], Step [350/1001], Loss: 4.9090, Perplexity: 135.4998, Time: 698.0621
Epoch [3/5], Step [400/1001], Loss: 4.9081, Perplexity: 135.3840, Time: 708.5022
Epoch [3/5], Step [450/1001], Loss: 4.5382, Perplexity: 93.5207, Time: 718.9354
Epoch [3/5], Step [500/1001], Loss: 4.5615, Perplexity: 95.7250, Time: 729.3580
Epoch [3/5], Step [550/1001], Loss: 4.5308, Perplexity: 92.8305, Time: 739.7726
Epoch [3/5], Step [600/1001], Loss: 4.4774, Perplexity: 88.0045, Time: 750.1894
Epoch [3/5], Step [650/1001], Loss: 4.8270, Perplexity: 124.8405, Time: 760.5896
Epoch [3/5], Step [700/1001], Loss: 4.5531, Perplexity: 94.9223, Time: 771.0343
Epoch [3/5], Step [750/1001], Loss: 4.4504, Perplexity: 85.6638, Time: 781.4623
Epoch [3/5], Step [800/1001], Loss: 4.5208, Perplexity: 91.9073, Time: 791.8763
Epoch [3/5], Step [850/1001], Loss: 4.5175, Perplexity: 91.6094, Time: 802.2961
Epoch [3/5], Step [900/1001], Loss: 4.7468, Perplexity: 115.2103, Time: 812.7055
Epoch [3/5], Step [950/1001], Loss: 4.4517, Perplexity: 85.7730, Time: 823.1708
Epoch [3/5], Step [1000/1001], Loss: 4.6192, Perplexity: 101.4103, Time: 833.5142
Epoch [4/5], Step [0/1001], Loss: 4.2618, Perplexity: 70.9386, Time: 834.1032
Epoch [4/5], Step [50/1001], Loss: 4.6852, Perplexity: 108.3358, Time: 844.4894
Epoch [4/5], Step [100/1001], Loss: 4.6809, Perplexity: 107.8666, Time: 854.9304
Epoch [4/5], Step [150/1001], Loss: 4.8973, Perplexity: 133.9251, Time: 865.3847
Epoch [4/5], Step [200/1001], Loss: 4.6673, Perplexity: 106.4129, Time: 875.8125
Epoch [4/5], Step [250/1001], Loss: 4.5725, Perplexity: 96.7837, Time: 886.2599
Epoch [4/5], Step [300/1001], Loss: 4.6393, Perplexity: 103.4708, Time: 896.7775
Epoch [4/5], Step [350/1001], Loss: 4.2560, Perplexity: 70.5276, Time: 907.2396
Epoch [4/5], Step [400/1001], Loss: 4.7077, Perplexity: 110.7923, Time: 917.6950
Epoch [4/5], Step [450/1001], Loss: 4.4915, Perplexity: 89.2528, Time: 928.1643
Epoch [4/5], Step [500/1001], Loss: 4.2909, Perplexity: 73.0313, Time: 938.5611
Epoch [4/5], Step [550/1001], Loss: 4.4751, Perplexity: 87.7996, Time: 948.9400
Epoch [4/5], Step [600/1001], Loss: 4.6531, Perplexity: 104.9086, Time: 959.3532
Epoch [4/5], Step [650/1001], Loss: 4.0892, Perplexity: 59.6906, Time: 969.7699
Epoch [4/5], Step [700/1001], Loss: 4.4164, Perplexity: 82.7958, Time: 980.1287
Epoch [4/5], Step [750/1001], Loss: 4.4471, Perplexity: 85.3758, Time: 990.4802
Epoch [4/5], Step [800/1001], Loss: 4.4442, Perplexity: 85.1327, Time: 1000.7544
Epoch [4/5], Step [850/1001], Loss: 4.6266, Perplexity: 102.1631, Time: 1011.1932
Epoch [4/5], Step [900/1001], Loss: 4.4920, Perplexity: 89.3032, Time: 1021.5972
Epoch [4/5], Step [950/1001], Loss: 4.6221, Perplexity: 101.7104, Time: 1032.0064
train_exp_lr.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  print '%s - loss: %.4f - time: %.4f'%(i, loss.data[0], time.time()-t0);
Epoch [4/5], Step [1000/1001], Loss: 4.2381, Perplexity: 69.2743, Time: 1042.2361
saving final model
1000 - loss: 4.2381 - time: 1042.2390
Namespace(batch_size=25, caption_path='./coco/annotations/sm_captions_train2014.json', crop_size=224, embed_size=256, hidden_size=512, image_dir='./data/resized2014', learning_rate=0.0001, log_step=50, model_path='./models/EXP2_LR', num_epochs=5, num_layers=1, num_workers=2, save_step=500, vocab_path='./data/vocab.pkl')
loading annotations into memory...
0:00:00.100986
creating index...
index created!
----- DATA_LOADER -- loaded data ----
---- USING GPU ---- 
train_exp_lr.py:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  return Variable(x, volatile=volatile)
train_exp_lr.py:96: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  loss.data[0], np.exp(loss.data[0]), time.time()-t0))
Epoch [0/5], Step [0/1001], Loss: 9.2053, Perplexity: 9949.9639, Time: 0.5023
Epoch [0/5], Step [50/1001], Loss: 7.9617, Perplexity: 2868.8242, Time: 10.2995
Epoch [0/5], Step [100/1001], Loss: 6.0613, Perplexity: 428.9242, Time: 20.1214
Epoch [0/5], Step [150/1001], Loss: 5.4576, Perplexity: 234.5351, Time: 30.0435
Epoch [0/5], Step [200/1001], Loss: 5.2842, Perplexity: 197.2035, Time: 40.6975
Epoch [0/5], Step [250/1001], Loss: 5.0327, Perplexity: 153.3398, Time: 51.2303
Epoch [0/5], Step [300/1001], Loss: 5.0073, Perplexity: 149.4975, Time: 61.7548
Epoch [0/5], Step [350/1001], Loss: 4.7835, Perplexity: 119.5167, Time: 72.2926
Epoch [0/5], Step [400/1001], Loss: 4.3720, Perplexity: 79.2047, Time: 82.7887
Epoch [0/5], Step [450/1001], Loss: 4.3951, Perplexity: 81.0558, Time: 93.2432
Epoch [0/5], Step [500/1001], Loss: 4.6549, Perplexity: 105.0967, Time: 103.7128
Epoch [0/5], Step [550/1001], Loss: 4.4353, Perplexity: 84.3801, Time: 114.2309
Epoch [0/5], Step [600/1001], Loss: 4.6274, Perplexity: 102.2479, Time: 124.6960
Epoch [0/5], Step [650/1001], Loss: 4.4334, Perplexity: 84.2184, Time: 135.1655
Epoch [0/5], Step [700/1001], Loss: 4.3765, Perplexity: 79.5560, Time: 145.6246
Epoch [0/5], Step [750/1001], Loss: 4.2059, Perplexity: 67.0823, Time: 156.0311
Epoch [0/5], Step [800/1001], Loss: 4.2852, Perplexity: 72.6174, Time: 166.4112
Epoch [0/5], Step [850/1001], Loss: 4.0093, Perplexity: 55.1109, Time: 176.8074
Epoch [0/5], Step [900/1001], Loss: 4.2506, Perplexity: 70.1473, Time: 187.3700
Epoch [0/5], Step [950/1001], Loss: 4.2510, Perplexity: 70.1767, Time: 197.8971
Epoch [0/5], Step [1000/1001], Loss: 4.0951, Perplexity: 60.0426, Time: 208.2844
Epoch [1/5], Step [0/1001], Loss: 3.7731, Perplexity: 43.5168, Time: 208.7821
Epoch [1/5], Step [50/1001], Loss: 4.4076, Perplexity: 82.0732, Time: 219.0748
Epoch [1/5], Step [100/1001], Loss: 4.0129, Perplexity: 55.3063, Time: 229.4427
Epoch [1/5], Step [150/1001], Loss: 3.7698, Perplexity: 43.3716, Time: 239.8084
Epoch [1/5], Step [200/1001], Loss: 3.9018, Perplexity: 49.4935, Time: 250.2507
Epoch [1/5], Step [250/1001], Loss: 4.0113, Perplexity: 55.2192, Time: 260.7449
Epoch [1/5], Step [300/1001], Loss: 3.7747, Perplexity: 43.5863, Time: 271.2003
Epoch [1/5], Step [350/1001], Loss: 3.9266, Perplexity: 50.7326, Time: 281.6229
Epoch [1/5], Step [400/1001], Loss: 3.8533, Perplexity: 47.1480, Time: 291.9954
Epoch [1/5], Step [450/1001], Loss: 3.6611, Perplexity: 38.9037, Time: 302.3740
Epoch [1/5], Step [500/1001], Loss: 4.0187, Perplexity: 55.6294, Time: 312.7540
Epoch [1/5], Step [550/1001], Loss: 3.8040, Perplexity: 44.8805, Time: 323.1621
Epoch [1/5], Step [600/1001], Loss: 4.1203, Perplexity: 61.5761, Time: 333.5352
Epoch [1/5], Step [650/1001], Loss: 3.6557, Perplexity: 38.6935, Time: 343.9070
Epoch [1/5], Step [700/1001], Loss: 3.5343, Perplexity: 34.2718, Time: 354.2513
Epoch [1/5], Step [750/1001], Loss: 3.7758, Perplexity: 43.6334, Time: 364.6489
Epoch [1/5], Step [800/1001], Loss: 3.5109, Perplexity: 33.4781, Time: 375.0047
Epoch [1/5], Step [850/1001], Loss: 3.7635, Perplexity: 43.0977, Time: 385.3340
Epoch [1/5], Step [900/1001], Loss: 3.6803, Perplexity: 39.6569, Time: 395.7274
Epoch [1/5], Step [950/1001], Loss: 3.8177, Perplexity: 45.4976, Time: 406.1331
Epoch [1/5], Step [1000/1001], Loss: 3.5871, Perplexity: 36.1300, Time: 416.3738
Epoch [2/5], Step [0/1001], Loss: 3.3106, Perplexity: 27.4007, Time: 416.8628
Epoch [2/5], Step [50/1001], Loss: 3.6802, Perplexity: 39.6538, Time: 427.1157
Epoch [2/5], Step [100/1001], Loss: 3.6600, Perplexity: 38.8607, Time: 437.4959
Epoch [2/5], Step [150/1001], Loss: 3.6361, Perplexity: 37.9438, Time: 447.8267
Epoch [2/5], Step [200/1001], Loss: 3.5627, Perplexity: 35.2571, Time: 458.2308
Epoch [2/5], Step [250/1001], Loss: 4.0839, Perplexity: 59.3745, Time: 468.6094
Epoch [2/5], Step [300/1001], Loss: 3.5935, Perplexity: 36.3599, Time: 478.9725
Epoch [2/5], Step [350/1001], Loss: 4.1678, Perplexity: 64.5728, Time: 489.3536
Epoch [2/5], Step [400/1001], Loss: 3.6292, Perplexity: 37.6821, Time: 499.6672
Epoch [2/5], Step [450/1001], Loss: 3.0547, Perplexity: 21.2157, Time: 510.0478
Epoch [2/5], Step [500/1001], Loss: 3.4588, Perplexity: 31.7777, Time: 520.3910
Epoch [2/5], Step [550/1001], Loss: 3.6507, Perplexity: 38.5009, Time: 530.7874
Epoch [2/5], Step [600/1001], Loss: 3.4245, Perplexity: 30.7085, Time: 541.1879
Epoch [2/5], Step [650/1001], Loss: 3.5653, Perplexity: 35.3516, Time: 551.5279
Epoch [2/5], Step [700/1001], Loss: 3.3186, Perplexity: 27.6207, Time: 561.9312
Epoch [2/5], Step [750/1001], Loss: 3.3932, Perplexity: 29.7600, Time: 572.3478
Epoch [2/5], Step [800/1001], Loss: 3.5326, Perplexity: 34.2124, Time: 582.7045
Epoch [2/5], Step [850/1001], Loss: 3.4143, Perplexity: 30.3968, Time: 593.1305
Epoch [2/5], Step [900/1001], Loss: 3.6978, Perplexity: 40.3573, Time: 603.5547
Epoch [2/5], Step [950/1001], Loss: 3.2783, Perplexity: 26.5313, Time: 613.9616
Epoch [2/5], Step [1000/1001], Loss: 3.6208, Perplexity: 37.3676, Time: 624.2727
Epoch [3/5], Step [0/1001], Loss: 3.2852, Perplexity: 26.7148, Time: 624.7770
Epoch [3/5], Step [50/1001], Loss: 3.3507, Perplexity: 28.5221, Time: 635.0187
Epoch [3/5], Step [100/1001], Loss: 3.3187, Perplexity: 27.6240, Time: 645.4084
Epoch [3/5], Step [150/1001], Loss: 3.1709, Perplexity: 23.8288, Time: 655.9280
Epoch [3/5], Step [200/1001], Loss: 3.2436, Perplexity: 25.6249, Time: 666.2224
Epoch [3/5], Step [250/1001], Loss: 3.4019, Perplexity: 30.0212, Time: 677.3424
Epoch [3/5], Step [300/1001], Loss: 3.2166, Perplexity: 24.9437, Time: 687.6693
Epoch [3/5], Step [350/1001], Loss: 3.2027, Perplexity: 24.5999, Time: 697.9123
Epoch [3/5], Step [400/1001], Loss: 3.5516, Perplexity: 34.8692, Time: 708.3972
Epoch [3/5], Step [450/1001], Loss: 3.3341, Perplexity: 28.0521, Time: 718.8332
Epoch [3/5], Step [500/1001], Loss: 3.4406, Perplexity: 31.2068, Time: 729.1971
Epoch [3/5], Step [550/1001], Loss: 3.3611, Perplexity: 28.8222, Time: 739.5550
Epoch [3/5], Step [600/1001], Loss: 3.4984, Perplexity: 33.0623, Time: 749.9806
Epoch [3/5], Step [650/1001], Loss: 3.0042, Perplexity: 20.1709, Time: 760.3762
Epoch [3/5], Step [700/1001], Loss: 3.2327, Perplexity: 25.3487, Time: 770.7745
Epoch [3/5], Step [750/1001], Loss: 3.5311, Perplexity: 34.1625, Time: 781.1648
Epoch [3/5], Step [800/1001], Loss: 3.1663, Perplexity: 23.7205, Time: 791.5092
Epoch [3/5], Step [850/1001], Loss: 3.5335, Perplexity: 34.2429, Time: 801.9379
Epoch [3/5], Step [900/1001], Loss: 3.0888, Perplexity: 21.9498, Time: 812.1937
Epoch [3/5], Step [950/1001], Loss: 3.2019, Perplexity: 24.5784, Time: 822.5587
Epoch [3/5], Step [1000/1001], Loss: 3.3397, Perplexity: 28.2110, Time: 832.8467
Epoch [4/5], Step [0/1001], Loss: 3.5012, Perplexity: 33.1560, Time: 833.4150
Epoch [4/5], Step [50/1001], Loss: 3.3365, Perplexity: 28.1219, Time: 843.7247
Epoch [4/5], Step [100/1001], Loss: 3.2737, Perplexity: 26.4091, Time: 854.1720
Epoch [4/5], Step [150/1001], Loss: 3.1208, Perplexity: 22.6647, Time: 864.6007
Epoch [4/5], Step [200/1001], Loss: 3.3023, Perplexity: 27.1745, Time: 874.9754
Epoch [4/5], Step [250/1001], Loss: 3.3996, Perplexity: 29.9528, Time: 885.3749
Epoch [4/5], Step [300/1001], Loss: 3.3354, Perplexity: 28.0892, Time: 895.8404
Epoch [4/5], Step [350/1001], Loss: 3.3278, Perplexity: 27.8771, Time: 906.2355
Epoch [4/5], Step [400/1001], Loss: 2.8648, Perplexity: 17.5461, Time: 916.6535
Epoch [4/5], Step [450/1001], Loss: 2.9633, Perplexity: 19.3619, Time: 927.0739
Epoch [4/5], Step [500/1001], Loss: 3.3742, Perplexity: 29.2004, Time: 937.4942
Epoch [4/5], Step [550/1001], Loss: 3.3113, Perplexity: 27.4207, Time: 947.9128
Epoch [4/5], Step [600/1001], Loss: 2.7412, Perplexity: 15.5058, Time: 958.3594
Epoch [4/5], Step [650/1001], Loss: 3.2223, Perplexity: 25.0865, Time: 968.7826
Epoch [4/5], Step [700/1001], Loss: 3.3186, Perplexity: 27.6216, Time: 979.2364
Epoch [4/5], Step [750/1001], Loss: 3.1427, Perplexity: 23.1666, Time: 989.7266
Epoch [4/5], Step [800/1001], Loss: 3.0644, Perplexity: 21.4214, Time: 1000.1787
Epoch [4/5], Step [850/1001], Loss: 2.7664, Perplexity: 15.9012, Time: 1010.6409
Epoch [4/5], Step [900/1001], Loss: 2.9522, Perplexity: 19.1489, Time: 1021.0999
Epoch [4/5], Step [950/1001], Loss: 3.1589, Perplexity: 23.5450, Time: 1031.5403
train_exp_lr.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  print '%s - loss: %.4f - time: %.4f'%(i, loss.data[0], time.time()-t0);
Epoch [4/5], Step [1000/1001], Loss: 3.0892, Perplexity: 21.9604, Time: 1041.9202
saving final model
1000 - loss: 3.0892 - time: 1041.9232
Namespace(batch_size=25, caption_path='./coco/annotations/sm_captions_train2014.json', crop_size=224, embed_size=256, hidden_size=512, image_dir='./data/resized2014', learning_rate=0.001, log_step=50, model_path='./models/EXP2_LR', num_epochs=5, num_layers=1, num_workers=2, save_step=500, vocab_path='./data/vocab.pkl')
loading annotations into memory...
0:00:00.111080
creating index...
index created!
----- DATA_LOADER -- loaded data ----
---- USING GPU ---- 
train_exp_lr.py:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  return Variable(x, volatile=volatile)
train_exp_lr.py:96: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  loss.data[0], np.exp(loss.data[0]), time.time()-t0))
Epoch [0/5], Step [0/1001], Loss: 9.2044, Perplexity: 9941.0859, Time: 0.5495
Epoch [0/5], Step [50/1001], Loss: 4.5229, Perplexity: 92.0989, Time: 10.3960
Epoch [0/5], Step [100/1001], Loss: 4.4711, Perplexity: 87.4551, Time: 20.2648
Epoch [0/5], Step [150/1001], Loss: 4.0040, Perplexity: 54.8151, Time: 30.1954
Epoch [0/5], Step [200/1001], Loss: 4.2404, Perplexity: 69.4356, Time: 40.8697
Epoch [0/5], Step [250/1001], Loss: 3.6811, Perplexity: 39.6910, Time: 51.4686
Epoch [0/5], Step [300/1001], Loss: 3.6884, Perplexity: 39.9808, Time: 62.0915
Epoch [0/5], Step [350/1001], Loss: 3.4948, Perplexity: 32.9436, Time: 72.5903
Epoch [0/5], Step [400/1001], Loss: 3.6745, Perplexity: 39.4308, Time: 83.0975
Epoch [0/5], Step [450/1001], Loss: 3.4954, Perplexity: 32.9647, Time: 93.6036
Epoch [0/5], Step [500/1001], Loss: 3.5830, Perplexity: 35.9825, Time: 104.1313
Epoch [0/5], Step [550/1001], Loss: 3.3759, Perplexity: 29.2520, Time: 114.6362
Epoch [0/5], Step [600/1001], Loss: 3.0379, Perplexity: 20.8624, Time: 125.2262
Epoch [0/5], Step [650/1001], Loss: 3.1058, Perplexity: 22.3277, Time: 135.7006
Epoch [0/5], Step [700/1001], Loss: 3.5314, Perplexity: 34.1726, Time: 146.1256
Epoch [0/5], Step [750/1001], Loss: 2.8551, Perplexity: 17.3755, Time: 156.4900
Epoch [0/5], Step [800/1001], Loss: 2.9999, Perplexity: 20.0837, Time: 166.9630
Epoch [0/5], Step [850/1001], Loss: 3.1529, Perplexity: 23.4034, Time: 177.4272
Epoch [0/5], Step [900/1001], Loss: 2.8860, Perplexity: 17.9216, Time: 187.8747
Epoch [0/5], Step [950/1001], Loss: 2.9211, Perplexity: 18.5610, Time: 198.3154
Epoch [0/5], Step [1000/1001], Loss: 2.9255, Perplexity: 18.6437, Time: 208.6375
Epoch [1/5], Step [0/1001], Loss: 2.5990, Perplexity: 13.4501, Time: 209.1686
Epoch [1/5], Step [50/1001], Loss: 2.3441, Perplexity: 10.4243, Time: 219.5296
Epoch [1/5], Step [100/1001], Loss: 3.0614, Perplexity: 21.3577, Time: 230.0403
Epoch [1/5], Step [150/1001], Loss: 3.2540, Perplexity: 25.8942, Time: 240.4721
Epoch [1/5], Step [200/1001], Loss: 2.7185, Perplexity: 15.1569, Time: 250.9072
Epoch [1/5], Step [250/1001], Loss: 2.8329, Perplexity: 16.9955, Time: 261.3475
Epoch [1/5], Step [300/1001], Loss: 2.7059, Perplexity: 14.9680, Time: 271.7971
Epoch [1/5], Step [350/1001], Loss: 2.8356, Perplexity: 17.0400, Time: 282.2186
Epoch [1/5], Step [400/1001], Loss: 3.0613, Perplexity: 21.3544, Time: 292.5923
Epoch [1/5], Step [450/1001], Loss: 2.6978, Perplexity: 14.8474, Time: 302.9731
Epoch [1/5], Step [500/1001], Loss: 2.8358, Perplexity: 17.0447, Time: 313.3650
Epoch [1/5], Step [550/1001], Loss: 2.3406, Perplexity: 10.3879, Time: 323.7612
Epoch [1/5], Step [600/1001], Loss: 2.9731, Perplexity: 19.5521, Time: 334.1821
Epoch [1/5], Step [650/1001], Loss: 2.5099, Perplexity: 12.3039, Time: 344.5509
Epoch [1/5], Step [700/1001], Loss: 2.6963, Perplexity: 14.8246, Time: 354.9554
Epoch [1/5], Step [750/1001], Loss: 2.7504, Perplexity: 15.6495, Time: 365.3481
Epoch [1/5], Step [800/1001], Loss: 2.9637, Perplexity: 19.3701, Time: 375.7376
Epoch [1/5], Step [850/1001], Loss: 2.5292, Perplexity: 12.5441, Time: 386.1519
Epoch [1/5], Step [900/1001], Loss: 2.6312, Perplexity: 13.8904, Time: 396.5179
Epoch [1/5], Step [950/1001], Loss: 2.4629, Perplexity: 11.7385, Time: 406.8921
Epoch [1/5], Step [1000/1001], Loss: 2.6648, Perplexity: 14.3650, Time: 417.1971
Epoch [2/5], Step [0/1001], Loss: 2.1380, Perplexity: 8.4824, Time: 417.6824
Epoch [2/5], Step [50/1001], Loss: 2.7862, Perplexity: 16.2190, Time: 428.0998
Epoch [2/5], Step [100/1001], Loss: 2.7169, Perplexity: 15.1338, Time: 438.4905
Epoch [2/5], Step [150/1001], Loss: 2.3127, Perplexity: 10.1013, Time: 448.8733
Epoch [2/5], Step [200/1001], Loss: 2.6184, Perplexity: 13.7135, Time: 464.0021
Epoch [2/5], Step [250/1001], Loss: 2.4554, Perplexity: 11.6509, Time: 478.2767
Epoch [2/5], Step [300/1001], Loss: 2.3184, Perplexity: 10.1597, Time: 489.6299
Epoch [2/5], Step [350/1001], Loss: 2.3535, Perplexity: 10.5222, Time: 501.8020
Epoch [2/5], Step [400/1001], Loss: 2.2746, Perplexity: 9.7239, Time: 512.4100
Epoch [2/5], Step [450/1001], Loss: 2.4792, Perplexity: 11.9314, Time: 522.8769
Epoch [2/5], Step [500/1001], Loss: 2.5991, Perplexity: 13.4511, Time: 533.4010
Epoch [2/5], Step [550/1001], Loss: 2.2506, Perplexity: 9.4938, Time: 543.8567
Epoch [2/5], Step [600/1001], Loss: 2.4555, Perplexity: 11.6526, Time: 554.2467
Epoch [2/5], Step [650/1001], Loss: 2.2687, Perplexity: 9.6671, Time: 564.7199
Epoch [2/5], Step [700/1001], Loss: 2.3340, Perplexity: 10.3195, Time: 575.1722
Epoch [2/5], Step [750/1001], Loss: 2.2982, Perplexity: 9.9564, Time: 585.5892
Epoch [2/5], Step [800/1001], Loss: 2.5532, Perplexity: 12.8478, Time: 596.0717
Epoch [2/5], Step [850/1001], Loss: 2.2465, Perplexity: 9.4546, Time: 606.5157
Epoch [2/5], Step [900/1001], Loss: 2.5229, Perplexity: 12.4648, Time: 616.9379
Epoch [2/5], Step [950/1001], Loss: 2.4672, Perplexity: 11.7892, Time: 627.3091
Epoch [2/5], Step [1000/1001], Loss: 2.3842, Perplexity: 10.8507, Time: 637.6100
Epoch [3/5], Step [0/1001], Loss: 2.1759, Perplexity: 8.8101, Time: 638.1243
Epoch [3/5], Step [50/1001], Loss: 2.1872, Perplexity: 8.9105, Time: 648.3860
Epoch [3/5], Step [100/1001], Loss: 2.2294, Perplexity: 9.2946, Time: 658.6873
Epoch [3/5], Step [150/1001], Loss: 2.2904, Perplexity: 9.8791, Time: 669.0822
Epoch [3/5], Step [200/1001], Loss: 2.0470, Perplexity: 7.7448, Time: 679.4247
Epoch [3/5], Step [250/1001], Loss: 2.0892, Perplexity: 8.0785, Time: 689.7992
Epoch [3/5], Step [300/1001], Loss: 2.1602, Perplexity: 8.6725, Time: 700.1833
Epoch [3/5], Step [350/1001], Loss: 2.3438, Perplexity: 10.4208, Time: 710.5040
Epoch [3/5], Step [400/1001], Loss: 2.1514, Perplexity: 8.5970, Time: 721.4979
Epoch [3/5], Step [450/1001], Loss: 1.9439, Perplexity: 6.9861, Time: 734.0455
Epoch [3/5], Step [500/1001], Loss: 1.9021, Perplexity: 6.6997, Time: 748.5649
Epoch [3/5], Step [550/1001], Loss: 2.0891, Perplexity: 8.0773, Time: 758.5699
Epoch [3/5], Step [600/1001], Loss: 2.3163, Perplexity: 10.1377, Time: 770.2369
Epoch [3/5], Step [650/1001], Loss: 2.2314, Perplexity: 9.3132, Time: 784.0300
Epoch [3/5], Step [700/1001], Loss: 2.4069, Perplexity: 11.0993, Time: 799.1627
Epoch [3/5], Step [750/1001], Loss: 2.3185, Perplexity: 10.1609, Time: 811.6466
Epoch [3/5], Step [800/1001], Loss: 2.1294, Perplexity: 8.4099, Time: 821.6648
Epoch [3/5], Step [850/1001], Loss: 2.1442, Perplexity: 8.5350, Time: 833.5013
Epoch [3/5], Step [900/1001], Loss: 2.1883, Perplexity: 8.9204, Time: 844.8591
Epoch [3/5], Step [950/1001], Loss: 2.0856, Perplexity: 8.0495, Time: 855.2491
Epoch [3/5], Step [1000/1001], Loss: 2.2813, Perplexity: 9.7895, Time: 868.1085
Epoch [4/5], Step [0/1001], Loss: 1.8960, Perplexity: 6.6593, Time: 868.6023
Epoch [4/5], Step [50/1001], Loss: 1.8748, Perplexity: 6.5198, Time: 878.9521
Epoch [4/5], Step [100/1001], Loss: 1.9387, Perplexity: 6.9495, Time: 889.3488
Epoch [4/5], Step [150/1001], Loss: 1.8109, Perplexity: 6.1160, Time: 899.7390
Epoch [4/5], Step [200/1001], Loss: 1.9288, Perplexity: 6.8813, Time: 910.1359
Epoch [4/5], Step [250/1001], Loss: 1.8618, Perplexity: 6.4350, Time: 920.5041
Epoch [4/5], Step [300/1001], Loss: 2.2232, Perplexity: 9.2369, Time: 930.8756
Epoch [4/5], Step [350/1001], Loss: 1.8298, Perplexity: 6.2323, Time: 941.2671
Epoch [4/5], Step [400/1001], Loss: 2.1142, Perplexity: 8.2830, Time: 952.8153
Epoch [4/5], Step [450/1001], Loss: 1.8789, Perplexity: 6.5464, Time: 963.1509
Epoch [4/5], Step [500/1001], Loss: 1.9632, Perplexity: 7.1221, Time: 973.5426
Epoch [4/5], Step [550/1001], Loss: 2.0264, Perplexity: 7.5871, Time: 984.0802
Epoch [4/5], Step [600/1001], Loss: 1.9087, Perplexity: 6.7440, Time: 994.4001
Epoch [4/5], Step [650/1001], Loss: 1.9951, Perplexity: 7.3533, Time: 1004.9501
Epoch [4/5], Step [700/1001], Loss: 2.0542, Perplexity: 7.8004, Time: 1015.3953
Epoch [4/5], Step [750/1001], Loss: 2.0038, Perplexity: 7.4173, Time: 1025.8595
Epoch [4/5], Step [800/1001], Loss: 2.0348, Perplexity: 7.6508, Time: 1036.3248
Epoch [4/5], Step [850/1001], Loss: 2.1258, Perplexity: 8.3796, Time: 1046.7697
Epoch [4/5], Step [900/1001], Loss: 1.8828, Perplexity: 6.5718, Time: 1057.2385
Epoch [4/5], Step [950/1001], Loss: 1.9708, Perplexity: 7.1765, Time: 1068.8242
train_exp_lr.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  print '%s - loss: %.4f - time: %.4f'%(i, loss.data[0], time.time()-t0);
Epoch [4/5], Step [1000/1001], Loss: 2.0118, Perplexity: 7.4765, Time: 1079.9818
saving final model
1000 - loss: 2.0118 - time: 1079.9846
Namespace(batch_size=25, caption_path='./coco/annotations/sm_captions_train2014.json', crop_size=224, embed_size=256, hidden_size=512, image_dir='./data/resized2014', learning_rate=0.01, log_step=50, model_path='./models/EXP2_LR', num_epochs=5, num_layers=1, num_workers=2, save_step=500, vocab_path='./data/vocab.pkl')
loading annotations into memory...
0:00:00.109773
creating index...
index created!
----- DATA_LOADER -- loaded data ----
---- USING GPU ---- 
train_exp_lr.py:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  return Variable(x, volatile=volatile)
train_exp_lr.py:96: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  loss.data[0], np.exp(loss.data[0]), time.time()-t0))
Epoch [0/5], Step [0/1001], Loss: 9.2076, Perplexity: 9972.6592, Time: 0.4966
Epoch [0/5], Step [50/1001], Loss: 4.0496, Perplexity: 57.3729, Time: 10.2575
Epoch [0/5], Step [100/1001], Loss: 3.7414, Perplexity: 42.1573, Time: 20.0985
Epoch [0/5], Step [150/1001], Loss: 2.8829, Perplexity: 17.8668, Time: 29.9619
Epoch [0/5], Step [200/1001], Loss: 3.3572, Perplexity: 28.7082, Time: 40.9435
Epoch [0/5], Step [250/1001], Loss: 3.1013, Perplexity: 22.2274, Time: 51.4473
Epoch [0/5], Step [300/1001], Loss: 3.0679, Perplexity: 21.4971, Time: 61.8239
Epoch [0/5], Step [350/1001], Loss: 3.0481, Perplexity: 21.0748, Time: 72.5083
Epoch [0/5], Step [400/1001], Loss: 3.0983, Perplexity: 22.1594, Time: 84.2281
Epoch [0/5], Step [450/1001], Loss: 3.4167, Perplexity: 30.4694, Time: 94.5597
Epoch [0/5], Step [500/1001], Loss: 3.2608, Perplexity: 26.0698, Time: 105.2198
Epoch [0/5], Step [550/1001], Loss: 3.4032, Perplexity: 30.0612, Time: 115.6711
Epoch [0/5], Step [600/1001], Loss: 3.2881, Perplexity: 26.7927, Time: 126.0629
Epoch [0/5], Step [650/1001], Loss: 2.9561, Perplexity: 19.2223, Time: 136.8916
Epoch [0/5], Step [700/1001], Loss: 2.8291, Perplexity: 16.9309, Time: 148.9577
Epoch [0/5], Step [750/1001], Loss: 3.0336, Perplexity: 20.7709, Time: 159.3151
Epoch [0/5], Step [800/1001], Loss: 2.7674, Perplexity: 15.9170, Time: 169.6861
Epoch [0/5], Step [850/1001], Loss: 3.4876, Perplexity: 32.7062, Time: 180.1238
Epoch [0/5], Step [900/1001], Loss: 2.6507, Perplexity: 14.1643, Time: 190.6533
Epoch [0/5], Step [950/1001], Loss: 2.6450, Perplexity: 14.0834, Time: 201.4348
Epoch [0/5], Step [1000/1001], Loss: 2.6531, Perplexity: 14.1985, Time: 211.7228
Epoch [1/5], Step [0/1001], Loss: 2.4212, Perplexity: 11.2593, Time: 212.6305
Epoch [1/5], Step [50/1001], Loss: 2.6574, Perplexity: 14.2593, Time: 222.9911
Epoch [1/5], Step [100/1001], Loss: 2.6807, Perplexity: 14.5959, Time: 233.8622
Epoch [1/5], Step [150/1001], Loss: 2.5587, Perplexity: 12.9191, Time: 245.4613
Epoch [1/5], Step [200/1001], Loss: 2.6024, Perplexity: 13.4964, Time: 259.2318
Epoch [1/5], Step [250/1001], Loss: 2.6824, Perplexity: 14.6200, Time: 269.4965
Epoch [1/5], Step [300/1001], Loss: 2.4369, Perplexity: 11.4372, Time: 279.9034
Epoch [1/5], Step [350/1001], Loss: 2.3619, Perplexity: 10.6106, Time: 290.3182
Epoch [1/5], Step [400/1001], Loss: 2.7455, Perplexity: 15.5720, Time: 300.7377
Epoch [1/5], Step [450/1001], Loss: 2.5503, Perplexity: 12.8104, Time: 311.1438
Epoch [1/5], Step [500/1001], Loss: 2.2445, Perplexity: 9.4353, Time: 321.7153
Epoch [1/5], Step [550/1001], Loss: 2.8846, Perplexity: 17.8969, Time: 336.6551
Epoch [1/5], Step [600/1001], Loss: 2.7967, Perplexity: 16.3898, Time: 346.7243
Epoch [1/5], Step [650/1001], Loss: 2.1606, Perplexity: 8.6764, Time: 357.1159
Epoch [1/5], Step [700/1001], Loss: 2.8113, Perplexity: 16.6317, Time: 369.9202
Epoch [1/5], Step [750/1001], Loss: 2.5298, Perplexity: 12.5506, Time: 383.6544
Epoch [1/5], Step [800/1001], Loss: 2.3996, Perplexity: 11.0189, Time: 393.5735
Epoch [1/5], Step [850/1001], Loss: 2.7493, Perplexity: 15.6320, Time: 403.9796
Epoch [1/5], Step [900/1001], Loss: 2.4166, Perplexity: 11.2077, Time: 414.4934
Epoch [1/5], Step [950/1001], Loss: 2.4128, Perplexity: 11.1650, Time: 425.7465
Epoch [1/5], Step [1000/1001], Loss: 2.7587, Perplexity: 15.7796, Time: 436.0412
Epoch [2/5], Step [0/1001], Loss: 2.3228, Perplexity: 10.2039, Time: 436.5360
Epoch [2/5], Step [50/1001], Loss: 2.2172, Perplexity: 9.1816, Time: 446.8113
Epoch [2/5], Step [100/1001], Loss: 2.5175, Perplexity: 12.3979, Time: 457.2046
Epoch [2/5], Step [150/1001], Loss: 2.2008, Perplexity: 9.0319, Time: 467.5595
Epoch [2/5], Step [200/1001], Loss: 2.1736, Perplexity: 8.7899, Time: 477.9589
Epoch [2/5], Step [250/1001], Loss: 2.3950, Perplexity: 10.9677, Time: 488.3788
Epoch [2/5], Step [300/1001], Loss: 2.7342, Perplexity: 15.3971, Time: 498.7566
Epoch [2/5], Step [350/1001], Loss: 2.3732, Perplexity: 10.7313, Time: 509.6396
Epoch [2/5], Step [400/1001], Loss: 2.3837, Perplexity: 10.8452, Time: 520.5122
Epoch [2/5], Step [450/1001], Loss: 2.3271, Perplexity: 10.2485, Time: 530.9559
Epoch [2/5], Step [500/1001], Loss: 2.5198, Perplexity: 12.4263, Time: 541.3418
Epoch [2/5], Step [550/1001], Loss: 2.5199, Perplexity: 12.4271, Time: 551.7454
Epoch [2/5], Step [600/1001], Loss: 2.2236, Perplexity: 9.2404, Time: 562.1393
Epoch [2/5], Step [650/1001], Loss: 2.3181, Perplexity: 10.1561, Time: 572.5627
Epoch [2/5], Step [700/1001], Loss: 2.3187, Perplexity: 10.1622, Time: 584.5059
Epoch [2/5], Step [750/1001], Loss: 2.1927, Perplexity: 8.9591, Time: 594.8889
Epoch [2/5], Step [800/1001], Loss: 2.5196, Perplexity: 12.4241, Time: 605.6662
Epoch [2/5], Step [850/1001], Loss: 2.5377, Perplexity: 12.6506, Time: 615.9834
Epoch [2/5], Step [900/1001], Loss: 2.2408, Perplexity: 9.4012, Time: 626.4013
Epoch [2/5], Step [950/1001], Loss: 2.2651, Perplexity: 9.6320, Time: 636.8130
Epoch [2/5], Step [1000/1001], Loss: 2.4374, Perplexity: 11.4430, Time: 647.1357
Epoch [3/5], Step [0/1001], Loss: 1.9971, Perplexity: 7.3674, Time: 647.6955
Epoch [3/5], Step [50/1001], Loss: 2.1123, Perplexity: 8.2669, Time: 657.9567
Epoch [3/5], Step [100/1001], Loss: 2.2413, Perplexity: 9.4055, Time: 668.3208
Epoch [3/5], Step [150/1001], Loss: 2.2456, Perplexity: 9.4457, Time: 678.7062
Epoch [3/5], Step [200/1001], Loss: 1.9534, Perplexity: 7.0528, Time: 689.1120
Epoch [3/5], Step [250/1001], Loss: 2.2119, Perplexity: 9.1328, Time: 699.5218
Epoch [3/5], Step [300/1001], Loss: 2.4121, Perplexity: 11.1569, Time: 709.9189
Epoch [3/5], Step [350/1001], Loss: 2.3581, Perplexity: 10.5708, Time: 720.2993
Epoch [3/5], Step [400/1001], Loss: 2.1267, Perplexity: 8.3874, Time: 730.6877
Epoch [3/5], Step [450/1001], Loss: 2.3364, Perplexity: 10.3438, Time: 741.1096
Epoch [3/5], Step [500/1001], Loss: 2.1997, Perplexity: 9.0220, Time: 751.5016
Epoch [3/5], Step [550/1001], Loss: 2.2002, Perplexity: 9.0267, Time: 763.1652
Epoch [3/5], Step [600/1001], Loss: 2.3834, Perplexity: 10.8418, Time: 776.7187
Epoch [3/5], Step [650/1001], Loss: 2.1934, Perplexity: 8.9657, Time: 787.5315
Epoch [3/5], Step [700/1001], Loss: 2.1983, Perplexity: 9.0099, Time: 797.9678
Epoch [3/5], Step [750/1001], Loss: 2.3292, Perplexity: 10.2702, Time: 808.4695
Epoch [3/5], Step [800/1001], Loss: 1.9723, Perplexity: 7.1873, Time: 819.0789
Epoch [3/5], Step [850/1001], Loss: 2.3206, Perplexity: 10.1823, Time: 830.7300
Epoch [3/5], Step [900/1001], Loss: 2.2920, Perplexity: 9.8945, Time: 841.1056
Epoch [3/5], Step [950/1001], Loss: 2.4904, Perplexity: 12.0664, Time: 851.5455
Epoch [3/5], Step [1000/1001], Loss: 2.1131, Perplexity: 8.2737, Time: 861.9154
Epoch [4/5], Step [0/1001], Loss: 2.0445, Perplexity: 7.7253, Time: 862.4945
Epoch [4/5], Step [50/1001], Loss: 2.3045, Perplexity: 10.0196, Time: 872.8452
Epoch [4/5], Step [100/1001], Loss: 2.1585, Perplexity: 8.6580, Time: 883.3361
Epoch [4/5], Step [150/1001], Loss: 2.1131, Perplexity: 8.2735, Time: 893.8301
Epoch [4/5], Step [200/1001], Loss: 1.9255, Perplexity: 6.8589, Time: 905.5166
Epoch [4/5], Step [250/1001], Loss: 2.2090, Perplexity: 9.1064, Time: 915.9999
Epoch [4/5], Step [300/1001], Loss: 2.2589, Perplexity: 9.5724, Time: 926.4586
Epoch [4/5], Step [350/1001], Loss: 2.3253, Perplexity: 10.2302, Time: 936.8974
Epoch [4/5], Step [400/1001], Loss: 1.9648, Perplexity: 7.1337, Time: 947.3700
Epoch [4/5], Step [450/1001], Loss: 2.0182, Perplexity: 7.5247, Time: 958.1220
Epoch [4/5], Step [500/1001], Loss: 2.3801, Perplexity: 10.8056, Time: 968.6534
Epoch [4/5], Step [550/1001], Loss: 2.1811, Perplexity: 8.8557, Time: 979.0549
Epoch [4/5], Step [600/1001], Loss: 2.0658, Perplexity: 7.8917, Time: 989.5184
Epoch [4/5], Step [650/1001], Loss: 2.0948, Perplexity: 8.1237, Time: 999.9877
Epoch [4/5], Step [700/1001], Loss: 1.9527, Perplexity: 7.0479, Time: 1010.4336
Epoch [4/5], Step [750/1001], Loss: 2.2154, Perplexity: 9.1650, Time: 1020.8732
Epoch [4/5], Step [800/1001], Loss: 2.0982, Perplexity: 8.1512, Time: 1031.6555
Epoch [4/5], Step [850/1001], Loss: 2.3493, Perplexity: 10.4785, Time: 1044.9711
Epoch [4/5], Step [900/1001], Loss: 2.3118, Perplexity: 10.0929, Time: 1057.5991
Epoch [4/5], Step [950/1001], Loss: 2.1736, Perplexity: 8.7896, Time: 1069.7346
train_exp_lr.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  print '%s - loss: %.4f - time: %.4f'%(i, loss.data[0], time.time()-t0);
Epoch [4/5], Step [1000/1001], Loss: 1.9654, Perplexity: 7.1381, Time: 1080.0776
saving final model
1000 - loss: 1.9654 - time: 1080.0844
Namespace(batch_size=25, caption_path='./coco/annotations/sm_captions_train2014.json', crop_size=224, embed_size=256, hidden_size=512, image_dir='./data/resized2014', learning_rate=0.1, log_step=50, model_path='./models/EXP2_LR', num_epochs=5, num_layers=1, num_workers=2, save_step=500, vocab_path='./data/vocab.pkl')
loading annotations into memory...
0:00:00.115842
creating index...
index created!
----- DATA_LOADER -- loaded data ----
---- USING GPU ---- 
train_exp_lr.py:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  return Variable(x, volatile=volatile)
train_exp_lr.py:96: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  loss.data[0], np.exp(loss.data[0]), time.time()-t0))
Epoch [0/5], Step [0/1001], Loss: 9.2087, Perplexity: 9983.2207, Time: 1.7308
Epoch [0/5], Step [50/1001], Loss: 8.1104, Perplexity: 3328.7957, Time: 13.6508
Epoch [0/5], Step [100/1001], Loss: 7.1010, Perplexity: 1213.1801, Time: 25.3505
Epoch [0/5], Step [150/1001], Loss: 7.4489, Perplexity: 1717.9891, Time: 35.2112
Epoch [0/5], Step [200/1001], Loss: 6.9586, Perplexity: 1052.1375, Time: 47.2745
Epoch [0/5], Step [250/1001], Loss: 5.6886, Perplexity: 295.4791, Time: 58.1716
Epoch [0/5], Step [300/1001], Loss: 4.6928, Perplexity: 109.1593, Time: 68.6470
Epoch [0/5], Step [350/1001], Loss: 4.9387, Perplexity: 139.5952, Time: 79.1770
Epoch [0/5], Step [400/1001], Loss: 5.3034, Perplexity: 201.0198, Time: 89.7101
Epoch [0/5], Step [450/1001], Loss: 5.1858, Perplexity: 178.7218, Time: 100.2651
Epoch [0/5], Step [500/1001], Loss: 4.9530, Perplexity: 141.5932, Time: 111.8376
Epoch [0/5], Step [550/1001], Loss: 4.7388, Perplexity: 114.2965, Time: 122.2028
Epoch [0/5], Step [600/1001], Loss: 5.1423, Perplexity: 171.1113, Time: 132.5599
Epoch [0/5], Step [650/1001], Loss: 4.5321, Perplexity: 92.9514, Time: 142.9986
Epoch [0/5], Step [700/1001], Loss: 6.0862, Perplexity: 439.7385, Time: 153.5057
Epoch [0/5], Step [750/1001], Loss: 5.3304, Perplexity: 206.5148, Time: 164.0999
Epoch [0/5], Step [800/1001], Loss: 5.4245, Perplexity: 226.8921, Time: 174.6136
Epoch [0/5], Step [850/1001], Loss: 5.3223, Perplexity: 204.8587, Time: 185.1054
Epoch [0/5], Step [900/1001], Loss: 4.9529, Perplexity: 141.5786, Time: 197.3705
Epoch [0/5], Step [950/1001], Loss: 5.3800, Perplexity: 217.0306, Time: 209.5516
Epoch [0/5], Step [1000/1001], Loss: 4.2074, Perplexity: 67.1788, Time: 222.6922
Epoch [1/5], Step [0/1001], Loss: 5.2440, Perplexity: 189.4276, Time: 223.2099
Epoch [1/5], Step [50/1001], Loss: 4.5474, Perplexity: 94.3848, Time: 233.3614
Epoch [1/5], Step [100/1001], Loss: 5.6053, Perplexity: 271.8530, Time: 243.7111
Epoch [1/5], Step [150/1001], Loss: 5.1304, Perplexity: 169.0868, Time: 254.1115
Epoch [1/5], Step [200/1001], Loss: 5.1549, Perplexity: 173.2816, Time: 264.5293
Epoch [1/5], Step [250/1001], Loss: 4.6062, Perplexity: 100.1037, Time: 274.9345
Epoch [1/5], Step [300/1001], Loss: 5.6087, Perplexity: 272.7798, Time: 285.3468
Epoch [1/5], Step [350/1001], Loss: 5.4091, Perplexity: 223.4237, Time: 295.7562
Epoch [1/5], Step [400/1001], Loss: 5.1580, Perplexity: 173.8079, Time: 306.3226
Epoch [1/5], Step [450/1001], Loss: 4.4451, Perplexity: 85.2101, Time: 316.8990
Epoch [1/5], Step [500/1001], Loss: 5.1806, Perplexity: 177.7852, Time: 327.3086
Epoch [1/5], Step [550/1001], Loss: 5.3053, Perplexity: 201.4075, Time: 337.7089
Epoch [1/5], Step [600/1001], Loss: 6.2121, Perplexity: 498.7642, Time: 348.1230
Epoch [1/5], Step [650/1001], Loss: 4.8208, Perplexity: 124.0627, Time: 358.5176
Epoch [1/5], Step [700/1001], Loss: 4.0277, Perplexity: 56.1325, Time: 368.8938
Epoch [1/5], Step [750/1001], Loss: 4.8728, Perplexity: 130.6820, Time: 379.2962
Epoch [1/5], Step [800/1001], Loss: 6.4674, Perplexity: 643.7930, Time: 389.7132
Epoch [1/5], Step [850/1001], Loss: 5.1963, Perplexity: 180.5954, Time: 400.1243
Epoch [1/5], Step [900/1001], Loss: 4.9852, Perplexity: 146.2256, Time: 410.5240
Epoch [1/5], Step [950/1001], Loss: 6.0521, Perplexity: 425.0144, Time: 420.9473
Epoch [1/5], Step [1000/1001], Loss: 5.6276, Perplexity: 278.0024, Time: 431.2958
Epoch [2/5], Step [0/1001], Loss: 5.4768, Perplexity: 239.0703, Time: 431.7427
Epoch [2/5], Step [50/1001], Loss: 5.6551, Perplexity: 285.7558, Time: 442.1181
Epoch [2/5], Step [100/1001], Loss: 4.4388, Perplexity: 84.6717, Time: 452.5557
Epoch [2/5], Step [150/1001], Loss: 4.8537, Perplexity: 128.2183, Time: 464.2570
Epoch [2/5], Step [200/1001], Loss: 4.8378, Perplexity: 126.1908, Time: 477.8936
Epoch [2/5], Step [250/1001], Loss: 4.4012, Perplexity: 81.5488, Time: 493.0566
Epoch [2/5], Step [300/1001], Loss: 5.0221, Perplexity: 151.7248, Time: 503.0155
Epoch [2/5], Step [350/1001], Loss: 5.5600, Perplexity: 259.8216, Time: 513.3859
Epoch [2/5], Step [400/1001], Loss: 3.5847, Perplexity: 36.0409, Time: 523.7958
Epoch [2/5], Step [450/1001], Loss: 4.6878, Perplexity: 108.6149, Time: 534.3449
Epoch [2/5], Step [500/1001], Loss: 5.9661, Perplexity: 389.9857, Time: 544.7926
Epoch [2/5], Step [550/1001], Loss: 5.8359, Perplexity: 342.3880, Time: 555.2577
Epoch [2/5], Step [600/1001], Loss: 6.0912, Perplexity: 441.9375, Time: 565.8440
Epoch [2/5], Step [650/1001], Loss: 5.0667, Perplexity: 158.6501, Time: 576.2377
Epoch [2/5], Step [700/1001], Loss: 5.3694, Perplexity: 214.7384, Time: 586.6180
Epoch [2/5], Step [750/1001], Loss: 5.7715, Perplexity: 321.0211, Time: 596.9814
Epoch [2/5], Step [800/1001], Loss: 4.0619, Perplexity: 58.0854, Time: 607.3524
Epoch [2/5], Step [850/1001], Loss: 6.4010, Perplexity: 602.4323, Time: 617.8054
Epoch [2/5], Step [900/1001], Loss: 5.2359, Perplexity: 187.9028, Time: 628.2837
Epoch [2/5], Step [950/1001], Loss: 4.5374, Perplexity: 93.4519, Time: 638.7060
Epoch [2/5], Step [1000/1001], Loss: 6.5028, Perplexity: 666.9834, Time: 649.0076
Epoch [3/5], Step [0/1001], Loss: 5.5522, Perplexity: 257.8001, Time: 649.5407
Epoch [3/5], Step [50/1001], Loss: 5.4738, Perplexity: 238.3525, Time: 659.7985
Epoch [3/5], Step [100/1001], Loss: 5.5143, Perplexity: 248.2056, Time: 670.0376
Epoch [3/5], Step [150/1001], Loss: 5.3282, Perplexity: 206.0639, Time: 681.3650
Epoch [3/5], Step [200/1001], Loss: 5.2234, Perplexity: 185.5732, Time: 692.2572
Epoch [3/5], Step [250/1001], Loss: 6.0741, Perplexity: 434.4411, Time: 702.5945
Epoch [3/5], Step [300/1001], Loss: 4.2572, Perplexity: 70.6136, Time: 712.9507
Epoch [3/5], Step [350/1001], Loss: 6.1921, Perplexity: 488.8817, Time: 723.3525
Epoch [3/5], Step [400/1001], Loss: 5.8648, Perplexity: 352.4179, Time: 733.6826
Epoch [3/5], Step [450/1001], Loss: 5.2623, Perplexity: 192.9327, Time: 744.0013
Epoch [3/5], Step [500/1001], Loss: 5.9182, Perplexity: 371.7576, Time: 756.0870
Epoch [3/5], Step [550/1001], Loss: 5.9973, Perplexity: 402.3512, Time: 767.9219
Epoch [3/5], Step [600/1001], Loss: 6.4242, Perplexity: 616.5627, Time: 778.7780
Epoch [3/5], Step [650/1001], Loss: 6.2531, Perplexity: 519.6194, Time: 789.1621
Epoch [3/5], Step [700/1001], Loss: 6.2510, Perplexity: 518.5140, Time: 799.5266
Epoch [3/5], Step [750/1001], Loss: 5.9076, Perplexity: 367.8330, Time: 809.8816
Epoch [3/5], Step [800/1001], Loss: 5.5156, Perplexity: 248.5357, Time: 820.2124
Epoch [3/5], Step [850/1001], Loss: 5.9646, Perplexity: 389.4082, Time: 830.6011
Epoch [3/5], Step [900/1001], Loss: 5.0371, Perplexity: 154.0264, Time: 840.9436
Epoch [3/5], Step [950/1001], Loss: 5.9784, Perplexity: 394.8197, Time: 851.3178
Epoch [3/5], Step [1000/1001], Loss: 5.2431, Perplexity: 189.2529, Time: 861.5918
Epoch [4/5], Step [0/1001], Loss: 6.0244, Perplexity: 413.3824, Time: 862.0998
Epoch [4/5], Step [50/1001], Loss: 5.1470, Perplexity: 171.9118, Time: 872.4066
Epoch [4/5], Step [100/1001], Loss: 5.1706, Perplexity: 176.0275, Time: 882.6860
Epoch [4/5], Step [150/1001], Loss: 6.5051, Perplexity: 668.5488, Time: 893.3001
Epoch [4/5], Step [200/1001], Loss: 4.8452, Perplexity: 127.1317, Time: 903.5357
Epoch [4/5], Step [250/1001], Loss: 5.8324, Perplexity: 341.1924, Time: 913.8760
Epoch [4/5], Step [300/1001], Loss: 6.5492, Perplexity: 698.6787, Time: 924.2661
Epoch [4/5], Step [350/1001], Loss: 5.9995, Perplexity: 403.2236, Time: 934.6573
Epoch [4/5], Step [400/1001], Loss: 6.6468, Perplexity: 770.3422, Time: 945.4892
Epoch [4/5], Step [450/1001], Loss: 6.6589, Perplexity: 779.6974, Time: 955.9393
Epoch [4/5], Step [500/1001], Loss: 5.8548, Perplexity: 348.9210, Time: 966.2741
Epoch [4/5], Step [550/1001], Loss: 5.4352, Perplexity: 229.3404, Time: 976.6730
Epoch [4/5], Step [600/1001], Loss: 6.0057, Perplexity: 405.7173, Time: 987.0800
Epoch [4/5], Step [650/1001], Loss: 5.8251, Perplexity: 338.6806, Time: 997.4261
Epoch [4/5], Step [700/1001], Loss: 5.6602, Perplexity: 287.2137, Time: 1007.8490
Epoch [4/5], Step [750/1001], Loss: 4.7018, Perplexity: 110.1494, Time: 1018.1927
Epoch [4/5], Step [800/1001], Loss: 6.3222, Perplexity: 556.7694, Time: 1028.5350
Epoch [4/5], Step [850/1001], Loss: 4.6786, Perplexity: 107.6223, Time: 1038.9707
Epoch [4/5], Step [900/1001], Loss: 6.1416, Perplexity: 464.7998, Time: 1049.4800
Epoch [4/5], Step [950/1001], Loss: 4.8951, Perplexity: 133.6319, Time: 1059.8152
train_exp_lr.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  print '%s - loss: %.4f - time: %.4f'%(i, loss.data[0], time.time()-t0);
Epoch [4/5], Step [1000/1001], Loss: 5.7604, Perplexity: 317.4634, Time: 1069.9668
saving final model
1000 - loss: 5.7604 - time: 1069.9700
Namespace(batch_size=25, caption_path='./coco/annotations/sm_captions_train2014.json', crop_size=224, embed_size=256, hidden_size=512, image_dir='./data/resized2014', learning_rate=1.0, log_step=50, model_path='./models/EXP2_LR', num_epochs=5, num_layers=1, num_workers=2, save_step=500, vocab_path='./data/vocab.pkl')
loading annotations into memory...
0:00:00.108280
creating index...
index created!
----- DATA_LOADER -- loaded data ----
---- USING GPU ---- 
train_exp_lr.py:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
  return Variable(x, volatile=volatile)
train_exp_lr.py:96: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  loss.data[0], np.exp(loss.data[0]), time.time()-t0))
Epoch [0/5], Step [0/1001], Loss: 9.2045, Perplexity: 9941.4746, Time: 0.5226
train_exp_lr.py:96: RuntimeWarning: overflow encountered in exp
  loss.data[0], np.exp(loss.data[0]), time.time()-t0))
Epoch [0/5], Step [50/1001], Loss: 106.5666, Perplexity:   inf, Time: 10.2826
Epoch [0/5], Step [100/1001], Loss: 136.6232, Perplexity:   inf, Time: 20.1173
Epoch [0/5], Step [150/1001], Loss: 147.5721, Perplexity:   inf, Time: 30.0057
Epoch [0/5], Step [200/1001], Loss: 160.7922, Perplexity:   inf, Time: 40.8972
Epoch [0/5], Step [250/1001], Loss: 146.1512, Perplexity:   inf, Time: 51.4081
Epoch [0/5], Step [300/1001], Loss: 138.2380, Perplexity:   inf, Time: 61.9108
Epoch [0/5], Step [350/1001], Loss: 165.9500, Perplexity:   inf, Time: 72.4305
Epoch [0/5], Step [400/1001], Loss: 166.4308, Perplexity:   inf, Time: 82.9629
Epoch [0/5], Step [450/1001], Loss: 157.5028, Perplexity:   inf, Time: 93.4404
Epoch [0/5], Step [500/1001], Loss: 189.6666, Perplexity:   inf, Time: 103.8001
Epoch [0/5], Step [550/1001], Loss: 202.8101, Perplexity:   inf, Time: 114.2872
Epoch [0/5], Step [600/1001], Loss: 164.6909, Perplexity:   inf, Time: 124.9145
Epoch [0/5], Step [650/1001], Loss: 185.9729, Perplexity:   inf, Time: 135.3597
Epoch [0/5], Step [700/1001], Loss: 205.7141, Perplexity:   inf, Time: 145.7917
Epoch [0/5], Step [750/1001], Loss: 181.0246, Perplexity:   inf, Time: 156.2452
Epoch [0/5], Step [800/1001], Loss: 158.6469, Perplexity:   inf, Time: 166.7147
Epoch [0/5], Step [850/1001], Loss: 234.4258, Perplexity:   inf, Time: 177.0923
Epoch [0/5], Step [900/1001], Loss: 181.7573, Perplexity:   inf, Time: 187.4641
Epoch [0/5], Step [950/1001], Loss: 211.8026, Perplexity:   inf, Time: 198.0047
Epoch [0/5], Step [1000/1001], Loss: 203.1213, Perplexity:   inf, Time: 208.4243
Epoch [1/5], Step [0/1001], Loss: 199.2729, Perplexity:   inf, Time: 209.8109
Epoch [1/5], Step [50/1001], Loss: 189.8372, Perplexity:   inf, Time: 219.9629
Epoch [1/5], Step [100/1001], Loss: 195.4126, Perplexity:   inf, Time: 230.3306
Epoch [1/5], Step [150/1001], Loss: 170.9275, Perplexity:   inf, Time: 240.7310
Epoch [1/5], Step [200/1001], Loss: 162.7255, Perplexity:   inf, Time: 251.1412
Epoch [1/5], Step [250/1001], Loss: 216.3242, Perplexity:   inf, Time: 261.6392
Epoch [1/5], Step [300/1001], Loss: 185.8405, Perplexity:   inf, Time: 272.0874
Epoch [1/5], Step [350/1001], Loss: 203.7604, Perplexity:   inf, Time: 282.4549
Epoch [1/5], Step [400/1001], Loss: 221.0606, Perplexity:   inf, Time: 292.8061
Epoch [1/5], Step [450/1001], Loss: 223.6796, Perplexity:   inf, Time: 303.1782
Epoch [1/5], Step [500/1001], Loss: 208.7707, Perplexity:   inf, Time: 313.5557
Epoch [1/5], Step [550/1001], Loss: 225.1840, Perplexity:   inf, Time: 323.9253
Epoch [1/5], Step [600/1001], Loss: 206.1724, Perplexity:   inf, Time: 334.2861
Epoch [1/5], Step [650/1001], Loss: 185.4993, Perplexity:   inf, Time: 344.8752
Epoch [1/5], Step [700/1001], Loss: 218.5123, Perplexity:   inf, Time: 355.3416
Epoch [1/5], Step [750/1001], Loss: 206.7920, Perplexity:   inf, Time: 365.7828
Epoch [1/5], Step [800/1001], Loss: 201.9501, Perplexity:   inf, Time: 376.2614
Epoch [1/5], Step [850/1001], Loss: 199.9533, Perplexity:   inf, Time: 386.7195
Epoch [1/5], Step [900/1001], Loss: 216.1880, Perplexity:   inf, Time: 397.1679
Epoch [1/5], Step [950/1001], Loss: 254.3952, Perplexity:   inf, Time: 407.6198
Epoch [1/5], Step [1000/1001], Loss: 195.7136, Perplexity:   inf, Time: 417.9423
Epoch [2/5], Step [0/1001], Loss: 180.2834, Perplexity:   inf, Time: 418.4983
Epoch [2/5], Step [50/1001], Loss: 190.9821, Perplexity:   inf, Time: 428.8775
Epoch [2/5], Step [100/1001], Loss: 173.1680, Perplexity:   inf, Time: 439.2819
Epoch [2/5], Step [150/1001], Loss: 198.7105, Perplexity:   inf, Time: 449.6844
Epoch [2/5], Step [200/1001], Loss: 182.7256, Perplexity:   inf, Time: 460.1195
Epoch [2/5], Step [250/1001], Loss: 222.5982, Perplexity:   inf, Time: 470.4502
Epoch [2/5], Step [300/1001], Loss: 195.3028, Perplexity:   inf, Time: 481.6986
Epoch [2/5], Step [350/1001], Loss: 218.9403, Perplexity:   inf, Time: 491.9556
Epoch [2/5], Step [400/1001], Loss: 220.6561, Perplexity:   inf, Time: 502.3298
Epoch [2/5], Step [450/1001], Loss: 213.5378, Perplexity:   inf, Time: 512.6935
Epoch [2/5], Step [500/1001], Loss: 241.3064, Perplexity:   inf, Time: 523.0697
Epoch [2/5], Step [550/1001], Loss: 228.3771, Perplexity:   inf, Time: 533.4333
Epoch [2/5], Step [600/1001], Loss: 239.4365, Perplexity:   inf, Time: 543.7543
Epoch [2/5], Step [650/1001], Loss: 199.6820, Perplexity:   inf, Time: 554.1633
Epoch [2/5], Step [700/1001], Loss: 175.3727, Perplexity:   inf, Time: 564.5033
Epoch [2/5], Step [750/1001], Loss: 211.8917, Perplexity:   inf, Time: 574.7716
Epoch [2/5], Step [800/1001], Loss: 250.1758, Perplexity:   inf, Time: 585.1630
Epoch [2/5], Step [850/1001], Loss: 185.4221, Perplexity:   inf, Time: 595.5305
Epoch [2/5], Step [900/1001], Loss: 245.3363, Perplexity:   inf, Time: 605.8998
Epoch [2/5], Step [950/1001], Loss: 207.8157, Perplexity:   inf, Time: 616.3198
Epoch [2/5], Step [1000/1001], Loss: 171.7323, Perplexity:   inf, Time: 626.6981
Epoch [3/5], Step [0/1001], Loss: 197.7525, Perplexity:   inf, Time: 627.8296
Epoch [3/5], Step [50/1001], Loss: 260.8386, Perplexity:   inf, Time: 637.9956
Epoch [3/5], Step [100/1001], Loss: 211.8244, Perplexity:   inf, Time: 648.6104
Epoch [3/5], Step [150/1001], Loss: 198.0178, Perplexity:   inf, Time: 660.6206
Epoch [3/5], Step [200/1001], Loss: 243.1736, Perplexity:   inf, Time: 671.9642
Epoch [3/5], Step [250/1001], Loss: 188.7513, Perplexity:   inf, Time: 682.8656
Epoch [3/5], Step [300/1001], Loss: 211.0723, Perplexity:   inf, Time: 693.1580
Epoch [3/5], Step [350/1001], Loss: 173.8043, Perplexity:   inf, Time: 703.5721
Epoch [3/5], Step [400/1001], Loss: 187.5056, Perplexity:   inf, Time: 713.9566
Epoch [3/5], Step [450/1001], Loss: 247.4205, Perplexity:   inf, Time: 724.2887
Epoch [3/5], Step [500/1001], Loss: 213.1012, Perplexity:   inf, Time: 734.6575
Epoch [3/5], Step [550/1001], Loss: 235.2223, Perplexity:   inf, Time: 745.1306
Epoch [3/5], Step [600/1001], Loss: 215.0490, Perplexity:   inf, Time: 755.5375
Epoch [3/5], Step [650/1001], Loss: 188.2524, Perplexity:   inf, Time: 766.4369
Epoch [3/5], Step [700/1001], Loss: 216.9190, Perplexity:   inf, Time: 776.8361
Epoch [3/5], Step [750/1001], Loss: 242.5939, Perplexity:   inf, Time: 787.1134
Epoch [3/5], Step [800/1001], Loss: 268.9009, Perplexity:   inf, Time: 797.4082
Epoch [3/5], Step [850/1001], Loss: 252.5402, Perplexity:   inf, Time: 807.7634
Epoch [3/5], Step [900/1001], Loss: 209.3369, Perplexity:   inf, Time: 818.0583
Epoch [3/5], Step [950/1001], Loss: 222.6165, Perplexity:   inf, Time: 828.3755
Epoch [3/5], Step [1000/1001], Loss: 243.7291, Perplexity:   inf, Time: 838.6398
Epoch [4/5], Step [0/1001], Loss: 233.2422, Perplexity:   inf, Time: 839.1630
Epoch [4/5], Step [50/1001], Loss: 229.7560, Perplexity:   inf, Time: 849.3456
Epoch [4/5], Step [100/1001], Loss: 232.0162, Perplexity:   inf, Time: 859.6029
Epoch [4/5], Step [150/1001], Loss: 227.9725, Perplexity:   inf, Time: 869.8316
Epoch [4/5], Step [200/1001], Loss: 211.8691, Perplexity:   inf, Time: 880.1111
Epoch [4/5], Step [250/1001], Loss: 240.1848, Perplexity:   inf, Time: 891.1195
Epoch [4/5], Step [300/1001], Loss: 227.2204, Perplexity:   inf, Time: 901.4714
Epoch [4/5], Step [350/1001], Loss: 283.7624, Perplexity:   inf, Time: 911.8499
Epoch [4/5], Step [400/1001], Loss: 238.5177, Perplexity:   inf, Time: 922.1091
Epoch [4/5], Step [450/1001], Loss: 227.1043, Perplexity:   inf, Time: 932.5053
Epoch [4/5], Step [500/1001], Loss: 200.7670, Perplexity:   inf, Time: 942.8456
Epoch [4/5], Step [550/1001], Loss: 225.6961, Perplexity:   inf, Time: 953.2238
Epoch [4/5], Step [600/1001], Loss: 199.5571, Perplexity:   inf, Time: 963.5001
Epoch [4/5], Step [650/1001], Loss: 261.5316, Perplexity:   inf, Time: 973.8980
Epoch [4/5], Step [700/1001], Loss: 212.3093, Perplexity:   inf, Time: 984.4527
Epoch [4/5], Step [750/1001], Loss: 217.0135, Perplexity:   inf, Time: 994.9597
Epoch [4/5], Step [800/1001], Loss: 247.0960, Perplexity:   inf, Time: 1005.3085
Epoch [4/5], Step [850/1001], Loss: 239.4772, Perplexity:   inf, Time: 1015.6506
Epoch [4/5], Step [900/1001], Loss: 255.5607, Perplexity:   inf, Time: 1026.0059
Epoch [4/5], Step [950/1001], Loss: 251.2796, Perplexity:   inf, Time: 1036.4541
train_exp_lr.py:100: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
  print '%s - loss: %.4f - time: %.4f'%(i, loss.data[0], time.time()-t0);
Epoch [4/5], Step [1000/1001], Loss: 209.0992, Perplexity:   inf, Time: 1048.8943
saving final model
1000 - loss: 209.0992 - time: 1048.8992
