{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "src_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.image_processor import default_image_transform\n",
    "from src.config import CaptionConfig\n",
    "from src.data_loader import CaptionDataManager\n",
    "from src.image_processor import train_transform\n",
    "from src.base_experiment import plot_eval\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "data = defaultdict(dict)\n",
    "\n",
    "Y_test = [x**2 for x in range(20)]\n",
    "Y_train = [math.sqrt(x) for x in range(20)]\n",
    "Y_val = [x for x in range(20)]\n",
    "\n",
    "data['loss']['train'] = Y_train\n",
    "data['loss']['test'] = Y_test\n",
    "data['loss']['val'] = Y_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_eval(data, 'train', name='loss')\n",
    "# plot_eval(data, 'test', name='loss')\n",
    "# plot_eval(data, 'val', name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nsimsiri/Documents/code/ml/nlp/cap/src/config.yml\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "loaded - val captions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 500/500 [00:00<00:00, 5008.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<src.data_loader.CaptionDataManager object at 0x1a21bd9160>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join(src_path, \"src\",\"config.yml\") \n",
    "print(config_path)\n",
    "assert(os.path.exists(config_path))\n",
    "\n",
    "manager = CaptionDataManager(config_path, n_sample=500)\n",
    "\n",
    "print(manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYPER-PARAM[vocab=1089 embed=64 hidden=128 num_layers=1 ]\n",
      "total param #: 440769\n",
      "decoder param #: 309505\n"
     ]
    }
   ],
   "source": [
    "from src.models.base_model import EncoderDecoder\n",
    "\n",
    "vocab_size      = len(manager.vocab())\n",
    "embed_size      = 64\n",
    "hidden_size     = 128\n",
    "num_layers      = 1\n",
    "\n",
    "num_epochs      = 10\n",
    "learning_rate   = 0.001\n",
    "\n",
    "\n",
    "print(\"HYPER-PARAM[vocab={} embed={} hidden={} num_layers={} ]\".format(vocab_size, \n",
    "                                                                       embed_size, \n",
    "                                                                       hidden_size,\n",
    "                                                                       num_layers))\n",
    "net = EncoderDecoder(vocab_size, \n",
    "                     embed_size=embed_size, \n",
    "                     hidden_size=hidden_size,\n",
    "                     num_layers=num_layers)\n",
    "\n",
    "total_param_count = sum(param.numel() for param in net.parameters())\n",
    "decoder_param_count = sum(param.numel() for param in net.decoder.parameters())\n",
    "print(\"total param #: \" + str(total_param_count))\n",
    "print(\"decoder param #: \" + str(decoder_param_count))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep=0 step=2 loss=6.99378228187561\n",
      "ep=0 step=4 loss=6.985867142677307\n",
      "ep=0 step=6 loss=6.976934274037679\n",
      "ep=0 step=8 loss=6.96873927116394\n",
      "ep=0 step=10 loss=6.9576562404632565\n",
      "ep=0 step=12 loss=6.94555926322937\n",
      "ep=0 step=14 loss=6.930100577218192\n",
      "ep=0 step=16 loss=6.9083079397678375\n",
      "ep=1 step=2 loss=6.622510671615601\n",
      "ep=1 step=4 loss=6.49799656867981\n",
      "ep=1 step=6 loss=6.378824234008789\n",
      "ep=1 step=8 loss=6.2600350975990295\n",
      "ep=1 step=10 loss=6.168941450119019\n",
      "ep=1 step=12 loss=6.0946158568064375\n",
      "ep=1 step=14 loss=6.040581635066441\n",
      "ep=1 step=16 loss=5.979053795337677\n",
      "ep=2 step=2 loss=5.347299098968506\n",
      "ep=2 step=4 loss=5.45111358165741\n",
      "ep=2 step=6 loss=5.403749863306682\n",
      "ep=2 step=8 loss=5.451522409915924\n",
      "ep=2 step=10 loss=5.457181978225708\n",
      "ep=2 step=12 loss=5.436914324760437\n",
      "ep=2 step=14 loss=5.423209837504795\n",
      "ep=2 step=16 loss=5.415204435586929\n",
      "ep=3 step=2 loss=5.207361221313477\n",
      "ep=3 step=4 loss=5.237094879150391\n",
      "ep=3 step=6 loss=5.229259570439656\n",
      "ep=3 step=8 loss=5.248616576194763\n",
      "ep=3 step=10 loss=5.237341356277466\n",
      "ep=3 step=12 loss=5.2545061111450195\n",
      "ep=3 step=14 loss=5.249708618436541\n",
      "ep=3 step=16 loss=5.245152115821838\n",
      "ep=4 step=2 loss=5.087459564208984\n",
      "ep=4 step=4 loss=5.100164890289307\n",
      "ep=4 step=6 loss=5.0939788818359375\n",
      "ep=4 step=8 loss=5.085176050662994\n",
      "ep=4 step=10 loss=5.084210443496704\n",
      "ep=4 step=12 loss=5.070640683174133\n",
      "ep=4 step=14 loss=5.07792980330331\n",
      "ep=4 step=16 loss=5.0720051527023315\n",
      "ep=5 step=2 loss=4.930800199508667\n",
      "ep=5 step=4 loss=4.935929656028748\n",
      "ep=5 step=6 loss=4.9295839468638105\n",
      "ep=5 step=8 loss=4.87548691034317\n",
      "ep=5 step=10 loss=4.863275766372681\n",
      "ep=5 step=12 loss=4.87311053276062\n",
      "ep=5 step=14 loss=4.875352893556867\n",
      "ep=5 step=16 loss=4.869162738323212\n",
      "ep=6 step=2 loss=4.763161659240723\n",
      "ep=6 step=4 loss=4.724581837654114\n",
      "ep=6 step=6 loss=4.685151735941569\n",
      "ep=6 step=8 loss=4.68837833404541\n",
      "ep=6 step=10 loss=4.675391435623169\n",
      "ep=6 step=12 loss=4.673799475034078\n",
      "ep=6 step=14 loss=4.67616469519479\n",
      "ep=6 step=16 loss=4.6722370982170105\n",
      "ep=7 step=2 loss=4.670226335525513\n",
      "ep=7 step=4 loss=4.568777561187744\n",
      "ep=7 step=6 loss=4.536967039108276\n",
      "ep=7 step=8 loss=4.557253956794739\n",
      "ep=7 step=10 loss=4.536354637145996\n",
      "ep=7 step=12 loss=4.519895156224568\n",
      "ep=7 step=14 loss=4.504126650946481\n",
      "ep=7 step=16 loss=4.5171829760074615\n",
      "ep=8 step=2 loss=4.4454052448272705\n",
      "ep=8 step=4 loss=4.453662395477295\n",
      "ep=8 step=6 loss=4.477979739507039\n",
      "ep=8 step=8 loss=4.452854156494141\n",
      "ep=8 step=10 loss=4.420444393157959\n",
      "ep=8 step=12 loss=4.404686013857524\n",
      "ep=8 step=14 loss=4.409206254141671\n",
      "ep=8 step=16 loss=4.406081855297089\n",
      "ep=9 step=2 loss=4.285521507263184\n",
      "ep=9 step=4 loss=4.254969596862793\n",
      "ep=9 step=6 loss=4.299707253774007\n",
      "ep=9 step=8 loss=4.339204728603363\n",
      "ep=9 step=10 loss=4.356157493591309\n",
      "ep=9 step=12 loss=4.341827829678853\n",
      "ep=9 step=14 loss=4.329571178981236\n",
      "ep=9 step=16 loss=4.329736560583115\n"
     ]
    }
   ],
   "source": [
    "from src.image_processor import train_transform, default_image_transform\n",
    "from src.base_experiment import plot_eval, run_model\n",
    "\n",
    "train_loader = manager.build_dataloader('train', \n",
    "                                        batch_size=25, \n",
    "                                        shuffle=True, \n",
    "#                                         image_transform=train_transform)\n",
    "                                        image_transform=default_image_transform)\n",
    "# val_loader   = manager.build_dataloader('val', \n",
    "#                                         batch_size=5)\n",
    "# test_loader  = manager.build_dataloader('test', \n",
    "#                                         batch_size=5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses, _ = run_model(net, \n",
    "                          train_loader, \n",
    "                          criterion, \n",
    "                          optimizer,\n",
    "                          train=True,\n",
    "                          epoch=epoch)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image torch.Size([1, 1, 64])\n",
      "torch.Size([1, 1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 1089])\n",
      "torch.Size([1])\n",
      "predicted workd:  tensor([[-2.0547, -1.2790,  8.2758,  ..., -1.7904, -1.4369, -2.0027]],\n",
      "       grad_fn=<AddmmBackward>) tensor([2]) <start>\n",
      "torch.Size([1, 1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 1089])\n",
      "torch.Size([1])\n",
      "predicted workd:  tensor([[-4.7995, -3.5948,  4.4539,  ..., -4.0931, -4.4234, -4.3693]],\n",
      "       grad_fn=<AddmmBackward>) tensor([12]) a\n",
      "torch.Size([1, 1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 1089])\n",
      "torch.Size([1])\n",
      "predicted workd:  tensor([[-4.2462, -3.1898,  0.6864,  ..., -3.6880, -3.6396, -3.8783]],\n",
      "       grad_fn=<AddmmBackward>) tensor([12]) a\n",
      "torch.Size([1, 1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 1089])\n",
      "torch.Size([1])\n",
      "predicted workd:  tensor([[-4.3487, -3.2769,  0.7099,  ..., -3.8351, -3.6822, -4.0930]],\n",
      "       grad_fn=<AddmmBackward>) tensor([12]) a\n",
      "torch.Size([1, 1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 1089])\n",
      "torch.Size([1])\n",
      "predicted workd:  tensor([[-4.5199, -3.4366,  0.4286,  ..., -4.0050, -3.8779, -4.2930]],\n",
      "       grad_fn=<AddmmBackward>) tensor([12]) a\n",
      "torch.Size([1, 1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 1089])\n",
      "torch.Size([1])\n",
      "predicted workd:  tensor([[-4.5356, -3.4593,  0.1412,  ..., -4.0052, -3.9346, -4.3485]],\n",
      "       grad_fn=<AddmmBackward>) tensor([12]) a\n",
      "torch.Size([1, 1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 1089])\n",
      "torch.Size([1])\n",
      "predicted workd:  tensor([[-4.5102, -3.4465, -0.0146,  ..., -3.9461, -3.9504, -4.3633]],\n",
      "       grad_fn=<AddmmBackward>) tensor([12]) a\n",
      "torch.Size([1, 1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 1089])\n",
      "torch.Size([1])\n",
      "predicted workd:  tensor([[-4.5027, -3.4761, -0.1139,  ..., -3.8848, -3.9626, -4.3828]],\n",
      "       grad_fn=<AddmmBackward>) tensor([12]) a\n",
      "torch.Size([1, 1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 1089])\n",
      "torch.Size([1])\n",
      "predicted workd:  tensor([[-4.5361, -3.5836, -0.1530,  ..., -3.8599, -3.9891, -4.4435]],\n",
      "       grad_fn=<AddmmBackward>) tensor([4]) .\n",
      "torch.Size([1, 1, 128]) torch.Size([1, 128])\n",
      "torch.Size([1, 1089])\n",
      "torch.Size([1])\n",
      "predicted workd:  tensor([[-5.5493, -4.6007,  1.3713,  ..., -4.6231, -5.1361, -5.3450]],\n",
      "       grad_fn=<AddmmBackward>) tensor([3]) <end>\n"
     ]
    }
   ],
   "source": [
    "from src.base_experiment import evaluate_model\n",
    "evaluate_model(net, train_loader, criterion, manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
