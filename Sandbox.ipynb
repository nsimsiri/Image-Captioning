{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'int32'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-c2d23b0869db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'int32'"
     ]
    }
   ],
   "source": [
    "torch.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  1\n",
      " 2  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "torch.Size([3, 4, 5])\n",
      "torch.Size([3, 5])\n",
      "3\n",
      "<class 'torch.nn.modules.activation.Softmax'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nsimsiri/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "softmax(): argument 'input' (position 1) must be Variable, not torch.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-9c60efc66ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0msoftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/nsimsiri/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0marg_reprs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             return ValueError('module.to expects .to(device), .to(dtype) or '\n\u001b[0;32m--> 357\u001b[0;31m                               \u001b[0;34m'.to(device, dtype), where dtype is a floating '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m                               \u001b[0;34m'point type, but got .to({})'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m                               .format(\", \".join(arg_reprs)))\n",
      "\u001b[0;32m/Users/nsimsiri/anaconda2/lib/python2.7/site-packages/torch/nn/modules/activation.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nsimsiri/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0mIn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mrrelu\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m \"\"\")\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m logsigmoid = _add_docstr(torch._C._nn.log_sigmoid, r\"\"\"\n",
      "\u001b[0;31mRuntimeError\u001b[0m: softmax(): argument 'input' (position 1) must be Variable, not torch.FloatTensor"
     ]
    }
   ],
   "source": [
    "a = torch.ones((3,5,2,2))\n",
    "b = torch.arange(3*5*2*2).view((3,5,2,2))\n",
    "print b[0,0]\n",
    "c = b.view((3,5,4));\n",
    "d = c.permute(0,2,1)\n",
    "# print c\n",
    "print d.shape\n",
    "a = torch.mean(d, 1)\n",
    "print a.shape\n",
    "f = a.unsqueeze(1);\n",
    "(n,_,_) = f.shape\n",
    "print n\n",
    "# print d.contiguous().view((12, 5)).shape\n",
    "\n",
    "print nn.Softmax\n",
    "softmax = nn.Softmax()\n",
    "print softmax(f);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "   0   1   2   3\n",
      "   4   5   6   7\n",
      "   8   9  10  11\n",
      "\n",
      "(1 ,.,.) = \n",
      "  12  13  14  15\n",
      "  16  17  18  19\n",
      "  20  21  22  23\n",
      "[torch.FloatTensor of size 2x3x4]\n",
      "\n",
      "\n",
      " 1  1  1\n",
      " 2  2  2\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  1\n",
      "  1\n",
      "  1\n",
      "\n",
      "(1 ,.,.) = \n",
      "  2\n",
      "  2\n",
      "  2\n",
      "[torch.FloatTensor of size 2x3x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(2*3*4).view((2,3,4))\n",
    "b = torch.Tensor(([[1,1,1],[2,2,2]]));\n",
    "\n",
    "print a\n",
    "print b\n",
    "print b.unsqueeze(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      "[torch.LongTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ensor([[1,10,5],[3,4,6]])\n",
    "k = torch.max(a, 1)\n",
    "print k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.LongTensor'>\n",
      "[[2, 2, 2, 2, 2]]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.LongTensor of size 5x1]\n",
      "\n",
      "\n",
      " 2  3\n",
      " 2  3\n",
      " 2  3\n",
      " 2  3\n",
      " 2  3\n",
      "[torch.LongTensor of size 5x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print torch.LongTensor\n",
    "a = [[2]*5]\n",
    "print a\n",
    "print torch.LongTensor(a).permute(1,0)\n",
    "a.append([3]*5)\n",
    "print torch.LongTensor(a).permute(1,0).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True],\n",
       "       [ True, False]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,0],[0,1]]);\n",
    "a == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
